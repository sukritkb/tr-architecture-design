# Medallion Architecture in Azure Synapse for Batch Data Processing

## Assumptions

From our discussions, I’m assuming that the cloud platform in use here is Azure.

---

## Overview

This architecture leverages **Azure Synapse** to build a **Medallion Architecture** (Bronze, Silver, and Gold layers) to process and transform data from an OLTP SQL Server database into an optimised structure for analytics. Each layer in this architecture serves a unique purpose, progressively enhancing data quality and usability. This design uses **Azure Synapse Spark Pools** for data transformation and **Copy Data Tool** for data ingestion, balancing performance, cost, and manageability. The different steps are orchestrated using **Azure Data Factory**.  

The goal is to replace all jobs that directly query the production database with queries directed at this structured data in the data lake.

---

## Architecture Layers and Data Flow

### 1. Bronze Layer: Raw Data Ingestion
   - **Objective**: Store raw, unprocessed data in Azure Data Lake Storage (ADLS) as a nightly copy from the OLTP database, retaining historical snapshots. We also apply retention policies to move older data to cold storage.
   - **Data Source**: OLTP SQL Server Database
   - **Storage Format**: Parquet
   - **Tools Used**: 
      - **Copy Data Tool** to perform a scheduled data copy from SQL Server to ADLS.
      - **Azure Data Factory** for orchestration of the different steps.
   - **Data Characteristics**: This layer captures raw data with minimal transformations to maintain a historical record, supporting compliance and future data reprocessing needs.

### 2. Silver Layer: Cleansed and Normalised Data
   - **Objective**: Transform raw data from the Bronze layer into a structured, clean, and normalised format (3NF-like). This layer includes "enterprise view" tables like master customers, markets, non-duplicated transactions and cross-reference tables.
   - **Data Source**: Bronze Layer (ADLS)
   - **Storage Format**: Parquet with Apache Hudi for efficient storage, supporting incremental updates.
   - **Tools Used**:
      - **Synapse Spark Pools** for data transformation and cleaning. 
      - **Azure Data Factory** for orchestration of the different steps.
   - **Data Characteristics**: Deduplication, null handling, and standardisation are applied. The data is organised into normalised tables by business domains (e.g., Customers, Markets, Location), preparing it for more complex aggregations and analytical processing.

### 3. Gold Layer: Star Schema for Analytics
   - **Objective**: Create a star schema optimised for analytical workloads, including BI and reporting, with fact and dimension tables for various use cases.
   - **Data Source**: Silver Layer (ADLS)
   - **Storage Format**: Azure Synapse Analytics Dedicated SQL Pool (high-performance data warehouse)
   - **Tools Used**:
      - **Synapse Spark Pools** for final transformations to a star schema to load data into Synapse Analytics Dedicated SQL Pool.
      - **Azure Data Factory** for orchestration of the different steps.
   - **Data Characteristics**: Fact and dimension tables are designed for efficient, high-performance querying in analytics and BI tools (e.g., Power BI). These tables are refreshed daily or according to the business’s reporting needs, ensuring current data availability.

---

## Step-by-Step Configuration and Setup

### Step 1: Setting Up the Bronze Layer
1. **Create Azure Data Lake Storage (ADLS)**: Set up a dedicated ADLS account for storing raw data.
2. **Use Copy Data Tool**:
   - **Create a new pipeline in Azure Data Factory**.
   - **Add a Copy Data activity** to the pipeline, connecting the source (SQL Server) and destination (ADLS).
   - **Set the copy schedule** to run nightly, ensuring data is ingested regularly.
3. **Define Data Retention Policies**: Configure policies in ADLS to move older data to cold storage after a defined period.

### Step 2: Transforming Data in the Silver Layer
1. **Access Bronze Layer Data**: Use Synapse Spark Pools to access the raw data stored in ADLS.
2. **Data Cleaning and Normalisation**:
   - Implement data transformation scripts using Spark to cleanse and normalise the data.
   - **Apply transformations** such as deduplication, null handling, and standardisation.
3. **Store Cleansed Data**:
   - Save the transformed data back to ADLS in Parquet format using Apache Hudi for efficient storage and incremental updates.
4. **Set Up Data Orchestration**: Use Azure Data Factory to schedule and monitor the transformation processes.

### Step 3: Creating the Gold Layer
1. **Load Data from Silver Layer**: Access the cleansed data from ADLS using Synapse Spark Pools.
2. **Design Star Schema**:
   - Define fact and dimension tables based on analytical requirements.
   - Implement transformation logic in Spark to convert cleansed data into a star schema structure.
3. **Load Data into Synapse Analytics**:
   - Use Spark to write the transformed star schema data into the Azure Synapse Analytics Dedicated SQL Pool.
4. **Refresh Data for BI Needs**: Configure daily refresh schedules or trigger refreshes based on business requirements to keep the data current using ADF.

---

## Key Design Choices and Considerations

### 1. Data Ingestion and Orchestration
   - **Tool Selection**: We use **Copy Data Tool** for the initial data copy from SQL Server to the Bronze layer (ADLS). Copy Data Tool is optimised for moving data from OLTP sources to storage with built-in scheduling, monitoring, and retry capabilities, making it cost-effective and reliable for periodic batch loads.
   - **Why Not Spark for Copy?**: While Synapse Spark can also copy data, Copy Data Tool is better suited for basic copy tasks because it is generally cheaper for straightforward data movement without transformations.
### 2. Data Transformation and Processing
   - **Tool Selection**: **Serverless Synapse Spark Pools** perform data transformation in the Silver and Gold layers. Spark is ideal for handling large volumes of data and performing complex transformations, such as normalisation and aggregation, required for the Silver and Gold layers.
   - **Why Spark?**:
      - **Scalability and Performance**: Spark’s distributed architecture allows for efficient data processing on large datasets, with in-memory capabilities to speed up transformations.
      - **ACID Transactions with Hudi**: Synapse Spark supports Apache Hudi, which enables ACID transactions, efficient upserts, and versioning, making it well-suited for building a Medallion Architecture.

### 3. Data Storage Formats
   - **Bronze and Silver Layers**: Data is stored in **Parquet format with Hudi** on ADLS. This format is optimised for analytics, supports compression, and integrates well with Synapse Spark. Apache Hudi adds support for incremental updates, time travel, and efficient querying on large datasets.
   - **Gold Layer**: Data is loaded into **Azure Synapse Analytics Dedicated SQL Pool**, which is a high-performance data warehouse optimised for large-scale analytical queries. This allows for faster BI reporting and reduces data movement, as Synapse SQL can be queried directly by tools like Power BI.

### 4. Medallion Architecture Benefits
   - **Layered Data Processing**: The Bronze, Silver, and Gold layers provide a structured approach to transforming raw data into high-quality, analytics-ready datasets.
   - **Data Quality and Governance**: Each layer progressively improves data quality and applies business logic, supporting data governance and traceability.
   - **Flexibility**: This architecture allows for both batch and real-time processing, with each layer’s data available for reprocessing or new use cases without impacting the original datasets.

### 5. Cost Management and Optimisation
   - **Copy Data Tool** for Data Movement**: Using Copy Data Tools (instead of Spark) for data ingestion to the Bronze layer minimises costs and is ideal for nightly batch processes that don’t require distributed processing.
   - **Spark Pools**: Synapse’s serverless model for Spark keeps costs under control by only charging for compute resources when transformations are running.
   - **Storage Optimisation with Parquet and Hudi**: Storing data in Parquet format and using Hudi optimises storage costs and improves query performance in the Bronze and Silver layers.

---

## Choice of Tools: Azure Synapse vs. Databricks

The primary choice for this architecture is to leverage **Azure Synapse** components (Copy Data Tool, Synapse Spark Pools, and Synapse SQL) to implement the batch processing pipeline. An alternative approach would be to use **Databricks** for the complete end-to-end pipeline, including data ingestion, transformation, and storage, alongside **Unity Catalog** for managing and registering tables. Here’s a comparison of the two approaches:

#### 1. Azure Synapse (Selected for this Design)

**Advantages**:
   - **Cost Efficiency**: Copy Data Tool for data movement and Spark Pools for transformation are often more cost-effective than Databricks for batch operations, especially when transformations are scheduled intermittently (e.g., nightly).
   - **Seamless Integration with Azure Ecosystem**: Synapse provides native integration with other Azure services, such as **Azure Data Lake Storage** (ADLS), **Azure SQL Database**, and **Power BI**, reducing the complexity of managing data flows and security across services.
   - **Reduced Vendor Lock-In**: By using Synapse, we’re staying within the Azure ecosystem without adding an additional layer of vendor lock-in from Databricks. This provides flexibility to pivot within Azure services without dependence on Databricks-specific features.
   - **Optimised for Data Warehousing**: With Synapse’s Dedicated SQL Pool (Gold layer), this architecture is optimised for large-scale analytics and BI workloads, providing high performance with a structured, analytical data store.

**Considerations**:
   - **Learning Curve**: This architecture leverages native Synapse components, which may require a learning curve and adaptation for teams familiar with Databricks or other platforms.
   - **Orchestration and Debugging**: Copy Data Tool and Spark Pools offer orchestration and monitoring but may require more hands-on management and setup compared to Databricks’ unified platform.

#### 2. Alternative: Databricks with Unity Catalog for End-to-End Pipeline

While Synapse is the selected approach, **Databricks** is a robust alternative for implementing the entire pipeline. Databricks offers a comprehensive, battle-tested platform that integrates ETL, data transformation, and data warehousing capabilities, especially suited for complex processing needs. Here’s an overview of how Databricks would compare:

**Advantages of Databricks**:
   - **Battle-Tested and Mature Platform**: Databricks is widely recognised as a leading solution for big data and analytics. It has proven stability, scalability, and performance for large-scale data pipelines, making it highly reliable for critical workloads.
   - **Low Maintenance with Managed Environment**: Databricks handles many of the infrastructure tasks (e.g., cluster management, autoscaling, and maintenance) automatically, reducing the operational burden on data engineering teams.
   - **Delta Lake for Enhanced Data Management**: Databricks’ Delta Lake offers ACID transactions, time travel, and efficient upserts/deletes, which make managing data quality and incremental updates simpler and more efficient.
   - **Unified Data Governance with Unity Catalog**: Unity Catalog provides centralised table and permissions management across workspaces, simplifying access control and security, particularly in multi-team environments.

**Considerations of Databricks**:
   - **Higher Cost**: Databricks can be more costly than Synapse, especially for less complex, batch-oriented workloads, as it requires dedicated cluster resources for both ingestion and transformation.
   - **Vendor Lock-In**: Leveraging Databricks involves an additional vendor lock-in beyond the cloud provider (Azure). Key features like Delta Lake and Unity Catalog are proprietary, which could limit flexibility if transitioning to another cloud provider or service.
   - **Integration with Azure Ecosystem**: While Databricks integrates well with ADLS and other Azure services, Synapse offers a more seamless, native experience within the Azure ecosystem.

#### Summary of My Experience and Architectural Choice

I have **significant experience working with Databricks** and have previously designed architectures using Databricks for end-to-end data processing. Databricks’ ease of use, low maintenance, and strong capabilities with Delta Lake and Unity Catalog have proven effective in past projects. In this design, I’m opting to use **Azure Synapse** as it provides a native solution within the Azure ecosystem. Synapse’s integration with services like **ADLS, Power BI, and Azure SQL Database** offers advantages for this use case and allows exploration of a highly integrated, seamless, cost-efficient Azure-native solution.

Another idea would be to try out Azure Fabric which Microsoft has been trying to push as the new unified Data Lake solution.   

Essentially, in both batch and streaming architectures, we are simply replacing the compute layer—Databricks—with Synapse Spark Pools. In my recent interactions with Microsoft solution architects, they have been promoting Synapse Spark Pools as a replacement for Databricks. One area where I think Databricks has an edge is in ease of use (UI) and how it's a unified data platform solution, though Synapse compensates with lower costs (I have seen Databricks costs spiral out of control). Additionally, many analysts find Synapse Analytics more comfortable to work with, as its traditional layout resembles a classic data warehouse, which feels more familiar compared to Databricks. We will discuss the streaming solution in the next document.

---

## Summary

This Medallion Architecture provides a structured, scalable approach for processing and transforming data from an OLTP source into a high-performance analytics environment on Azure Synapse. By leveraging Copy Data Tool for ingestion, Spark for transformation, and Synapse SQL for analytics, this design meets the needs for high data quality, scalability, and cost efficiency while ensuring readiness for BI and analytical workloads.

**Key Benefits**:
   - Efficient data ingestion and transformation using Azure Synapse.
   - Optimised storage and querying capabilities with ADLS, Hudi, and Synapse SQL.
   - Supports future scaling and adaptability for additional data use cases.

---

## References

[Lakehouse Data Format Comparisons](https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison)  
[Comparing Synapse and Databricks](https://learn.microsoft.com/en-us/data-engineering/playbook/articles/databricks-vs-synapse)  
[Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)  
[Synapse Analytics](https://azure.microsoft.com/en-gb/products/synapse-analytics)
