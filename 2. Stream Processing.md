## Implementation Details

The goal with this architecture is to replace ***"continously"*** running jobs which directly query the SQL databases.

### Step-by-Step Configuration and Setup

1. **Setting Up Change Data Capture (CDC) on the OLTP Database**:
   - **Enable CDC**: Configure Change Data Capture (CDC) on the OLTP SQL Server database to capture inserts, updates, and deletes. 
   - **Configure Data Flow to Event Hubs**: Use **Azure Data Factory (ADF) Mapping Data Flow** or **native SQL Server CDC** capabilities to stream CDC changes to Event Hubs.
   - **Event Schema**: Structure events in JSON format, containing key fields (such as operation type, timestamp, and record ID) to track changes.

2. **Configuring Azure Event Hubs**:
   - **Create Event Hub Namespace and Instance**: Set up an Event Hub instance to handle incoming CDC data.
   - **Partition Strategy**: Configure partitions in Event Hubs to handle expected data throughput. Partitioning enables parallel processing and improves read performance for Spark.
   - **Data Retention Policy**: Set retention policies based on data consumption frequency. This ensures data is available for reprocessing if necessary.

3. **Setting Up Synapse Spark Pools for Stream Processing**:
   - **Structured Streaming in Spark**: Use Synapse Spark Pools to read data from Event Hubs using Spark’s Structured Streaming API.
   - **Event Processing Logic**: Implement transformation logic in Spark to filter, deduplicate, and format events. For example, consolidate inserts and updates into a final “upsert” state before writing to Cosmos DB.
   - **Error Handling**: Implement error handling and logging for transformation errors and data inconsistencies within the Spark job, ensuring resilience and troubleshooting support.
   - **Batch and Trigger Intervals**: Configure batch intervals in Spark Structured Streaming for real-time responsiveness without overwhelming Cosmos DB or other downstream systems.

4. **Configuring Cosmos DB for Real-Time Data Storage** (Thinking of Cosmos as Cassandra):
   - **Database and Collection Setup**: Set up Cosmos DB with collections to store the final data state.
   - **Partition Key Design**: Choose partition keys to optimise performance based on expected query patterns. For example, if most queries are by customer ID, consider using customer ID as the partition key.
   - **Consistency Model**: Use a consistency level that balances read latency with accuracy, such as **Session Consistency** for most real-time use cases, ensuring predictable performance.
   - **Indexing Policy**: Customise indexing in Cosmos DB to optimise for query speed and cost, indexing only required fields.

---

## Key Design Choices and Considerations

1. **Use of Azure Event Hubs**: Chose Azure Event Hubs for its high throughput and ability to handle large volumes of streaming data efficiently. It supports partitioning, allowing for parallel processing, which is essential for scaling as data volumes grow. Event Hubs also integrates seamlessly with other Azure services, facilitating a smooth data flow within the Azure ecosystem.

2. **Structured Streaming in Spark**: Utilising Spark’s Structured Streaming API allows for real-time data processing and easy integration with various data sources and sinks. The ability to handle unbounded data streams makes it suitable for the requirement to process CDC changes continuously.

3. **Cosmos DB as a Real-Time Store**: Cosmos DB was selected for its low-latency data access and scalability. Its multi-model support and ability to scale throughput and storage independently provide the flexibility needed for evolving application requirements. By carefully designing partition keys based on access patterns, we ensure efficient query performance.

4. **Error Handling and Resilience**: The architecture emphasises error handling within Spark jobs to ensure data integrity and robustness. Implementing detailed logging and monitoring mechanisms enables quick identification and resolution of issues, promoting system reliability.

5. **Batch and Trigger Interval Configuration**: We strategically configure batch intervals to balance processing speed and system resource usage, preventing overloads on downstream systems like Cosmos DB. This design choice aids in maintaining high availability and responsiveness in data processing.

6. **Retention Policies in Event Hubs**: The decision to set data retention policies in Event Hubs was made to accommodate reprocessing needs without losing data integrity. This flexibility is crucial for debugging and validating data streams as the system evolves.


---

## Summary

This real-time infrastructure in Azure Synapse offers a scalable, Azure-native solution for handling CDC data, transforming it in real time, and storing it in Cosmos DB for fast, low-latency access. By leveraging **Azure Event Hubs** and **Synapse Spark Pools** for streaming, the architecture efficiently handles high-velocity data with near real-time updates, reducing the burden on OLTP systems and enabling responsive analytics. 

While Databricks offers advanced features like Delta Lake for real-time processing, the Azure Synapse-based design chosen here emphasises close integration with the Azure ecosystem, cost efficiency, and flexibility. This architecture provides a foundation for expanding into real-time analytics, machine learning, and hybrid batch-streaming use cases in the future.

This design allows for flexibility, scalability, and operational efficiency, aligning with business needs for a robust, integrated, and Azure-native real-time processing solution.

---

**Note**: Although I haven’t implemented this architecture in Azure (i.e in my current role) --, I have built similar real-time streaming architectures at my previous organisations. In those environments, I used **Databricks** with continuously running Spark clusters to read data streams from **Kafka clusters**, process and calculate real-time metrics, and write results back to Kafka topics or store it in **Cassandra tables** for real-time analytics for a customer segmentation platform. This Azure-based architecture leverages similar principles but is designed to take advantage of Azure-native services.